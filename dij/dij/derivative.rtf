{\rtf1\mac\ansicpg10000\cocoartf824\cocoasubrtf480
{\fonttbl\f0\fswiss\fcharset77 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww16360\viewh13520\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural

\f0\fs24 \cf0 \
;(send canvas render-scene (lambda (x) (row-sum x '(0 0 420) ) ) ) \
\
;(1-tx)^2 * p(0,0) + 2*tx*(1-tx) * c(0,0, 1,0) + tx^2 * p(1,0)  for ty = 0\
;(1-ty)^2 * p(0,0) + 2*ty*(1-ty) * c(0,0, 0,1) + ty^2 * p(0,1)  for tx = 0\
;(tx)^2 * p(1,0) + 2*ty*(tx) * c(1,0, 0,1) + ty^2 * p(0,1)  for tx+ty = 1\
;  \
;coefficient of tx^2 is p(1,0) for tx != 0\
;coefficient of ty^2 is p(0,1) for ty != 0\
;because I can make the substitution 1-ty = tx\
  \
;for an arbitrary line a*tx + b*ty = 1\
\
;the ultimate goal is to ensure that for any two points on the boundry, the curve between them, \
;lying in the plane generated by the normal vectors is a Bezier curve.  The normals on the boundry are \
;unknown except at the corners.  For the condition to be satisfied at the corners, it is sufficient to \
;place the control point in the plane generated by the normal vectors.  Once we have done that, the normal\
;  vector of the boundry is known (because it lies in the same plane).  (wait that can't be right ... )\
                                                                     \
;we know that when two planes in which these curves lie meet, the curves themselves must meet at the same place.\
  \
;p(1,0) * tx^2 + p(0,1) * ty^2 + 2*(tx)*(1-tx)*ty*(1-ty) * C\
 \
;a general Bezier surface is of the form \
     ; tx^2 * ty^2 * C(0,0) + tx^2 * 2*ty*(1-ty) * C(0,1) + tx^2*(1-ty)^2 * C(0,2) +\
     ; 2*tx*(1-tx) * ty^2 * C(1, 0) + 2*tx*(1-tx) * 2*ty*(1-ty) * C(1, 1) + ...\
;So when ty == 0 we have : tx^2 * C(0,2) + 2*tx*(1-tx)* C(1, 2) + (1-tx)^2 * C( 2, 2)\
;   when tx == 0 we have : ty^2 * C( 2,0)  + 2*ty*(1-ty) *  C(2,1) + (1-ty)^2 * C(2, 2) <-- note that the meet at this control point\
;   when tx+ty == 1 we have either :\
;   tx^2 * (1-tx)^2  * C(0,0) + tx^2 * 2 * (1-tx)*tx * C(0,1) + tx^2*tx^2 * C(0,2) +\
;   2*tx*(1-tx) * (1-tx) *(1-tx)^2 * C(1,0) + 2*tx(1-tx) * 2*tx(1-tx) * C(1,1) ...\
;   this too is a Bezier curve with parameter as sqrt(tx) ?\
\
\
\
;we find the normals to this surface by differentiating with respect to tx, differentiating with respect to ty and then taking a cross product of the result.\
\
;consider a case in which the objects under consideration are lines through parameter space.\
;still, the space is two dimensional, as every line is defined by a slope and an intercept except one.  we can parameterize the whole space by giving the two end points\
;as distances around the one dimensional boundry of the triangle, but that overspecifies.  Ideally the boundry of the triangle would provide the corners of a new triangle\
;in the line space : (0,0) (0,1) (1,0).  For reasons of symetry let us place each corner oposite the line it represents :\
;(0,0) -> (0,1)--(1,0)\
;(0,1) -> (0,0)--(1,0)\
;(1,0) -> (0,1)--(0,0)\
;varying the second parameter from 0 to 1 changes only the first end point.  All those lines go through the corner (1,0)\
;varying the first parameter from 0 to 1 changes only the other end point. All those lines go through the corner (0,1)\
;(1,1) would be degenerate.  Is there any way to force the equation x+y <= 1 ? \
;for the sake of symmetry, lines passing through the origin should have x+y = 1.\
;so for very small d the line (0+d, 1-d) should pass through the origin, and lie close to the line (0,1-d) which passes through (1,0), but not through the origin\
;the general equation for a line in space takes the form : ax + by + c = 0 with a = 0 a horizontal line and b = 0 a vertical line.\
;this also is too specific since multiplication of the coefficients by a constant factor gives the same line.\
;our line will be f(x,y)X + g(x,y)Y + h(x,y) = 0. with x+y <= 1\
; x+y==1 -> h(x,y) = 0 .lines with x+y=1 pass through the origin\
; f(x,y)==0 && h(x,y)==0 -> x=0 && y=1 \
; g(x,y)==0 && h(x,y)==0 -> y=0 && x=1\
; x=0 -> f(x,y) + h(x,y) = 0 \
; y=0 -> g(x,y) + h(x,y) = 0\
; conjecture : (x,0) returns the line (0,1) -- (1-x, 0 )  which is X = (1-x) + Y*(x-1) \
;              (0,y) returns the line (1,0) -- ( 0, 1-y ) which is Y = (1-y) + X*(y-1)\
;              (x, 1-x) returns the line (0,0) -- (1-x, x) which is X*(x) + Y*(1-x) = 0\
;              f(x,0) = 1 , f(0,y) = (y-1) , f(x, 1-x) = x\
; ???? enough of that\
; the surface is a function from the parameter space to the three dimensional space F(u,v) = (x,y,z)\
; the value F(0, v) F(u, 0) and F(q, 1-q) are known.  The goal is to find partial derivatives dF/du and dF/dv at all the points\
; on the boundry by interpolating between the corners.\
; at (0,0) this task is easy.  at the other corners we know the partial in one direction, and the rest we have to get by differentiating\
; with respect to the parameter q.  dF/dq = dF/du * du/dq + dF/dv * dv/dq . dF/dq is known, du/dq = 1, dv/dq = -1 \
; along the y axis where dF/dv is known dF/du = dF/dq + dF/dv, along the x axis where dF/du is known dF/dv = dF/dq - dF/du \
\
;if we assume that, along the x and y axis, the partial derivates are found by linear interpolation,\
;then the second derivatives dF/dudv and dF/dvdu at the origin are constant (and I think they are supposed to be the same)\
             ;the derivate dF/dudv is found differentiating dF/du(v) = dF/du(0,0) * (1-v) + dF/du(0,1) * v\
             ;dF/dudv = dF/du(1)-dF/du(0) \
             ;likewise dF/dvdu = dF/dv(1)-dF/dv(0)\
             ;are they equal?\
             ;dF/du = dF/dq + dF/dv [where v = 0]\
             ;dF/dv = dF/dq - dF/du [where u = 0]\
             ;dF/dudv(0,0) = dF/du(1)-dF/du(0) = dF/dq + dF/dv - dF/du\
             ;dF/dvdu(0,0) = dF/dv(1)-dF/dv(0) = dF/dq - dF/du -? dF/dv\
\
;given that dF/dvdu = dF/dudv = C\
;dF/du = S( dF/dudv ) dv = C*v + Kuv(u) = S( dF/dudu ) du = dF/dudu*u + Kuu(v)\
;dF/dv = S( dF/dvdu ) du = C*u + Kvu(v) = S( dF/dvdv ) dv = dF/dvdv*v + Kvv(u)\
; F = S( dF/du ) du = C*v*u + Kuv(v)*u + K0 = dF/dudu*u^2/2 + Kuu(v)*u + Ku(v) + K0 = \
;     S( dF/dv ) dv = C*v*u + Kvu(u)*v + K0 = dF/dvdv*v^2/2 + Kvv(u)*v + Kv(u) + K0\
;   = dF/dudv*v*u + dF/dudu*u^2/2 + dF/dvdv*v^2/2 + dF/dv*v + dF/dv*v + dF/du*u + K0\
\
F  = 1/2 uu Cuu + 1/2 vv Cvv + uv Cuv + u Cu + v Cv + C0\
\
dF/dv = v Cvv + u Cuv + Cv\
\
dF/dv (1, 0) = Cuv + Cv = \
                        (\
                        [- dB(p2 c2 p0)(0) - dB(p1 c1 p2)(1) ] - dB(p0 c0 p1)(0)\
                        ) + \
                        dB(p0 c0 p1)(0)\
\
now to solve the equation V * N = 0 where V is constant and \
          N = dF/du(u,v) x dF/dv(u,v) =\
                ( u Cuu  + v Cuv + Cu ) x\
                ( v Cvv +  u Cuv + Cv ) = \
                        uv (Cuu x Cvv) +  \
                        uu(Cuu x Cuv) +\
                        u(Cuu x Cv ) + \
                        vv( Cuv x Cvv ) + \
                        vu(Cuv x Cuv ) +  -- = 0\
                        v(Cuv x Cv ) +\
                        v(Cu x Cvv) +\
                        u(Cu x Cuv) +\
                           (Cu x Cv)\
\
differentiating with respect to u\
v( Cuu x Cvv ) + \
2u(Cuu x Cuv ) + \
(Cuu x Cv ) + ( Cu x Cuv ) = 0\
\
differentiating with respect to v\
u(Cuu x Cvv)\
2v( Cuv x Cvv ) +\
(Cuv x Cv ) + ( Cu x Cvv ) = 0\
\
in two dimensions, a vector perpendicular to (x,y) is (-y, x)\
in three dimensions a vector perpendicular to (x,y,z)  \
\
is it true that the image of any line in parameter space lies in a plane in vector space?\
I think so, because the second derivative is a constant.\
\
the quality of being perpendicular to both partial derivatives represents a system of equations of the following form :\
\
dF/du (t) * x = 0\
dF/dv (t) * x = 0\
\
the question now is : given that dF/du and dF/dv are linear functions of t , and that the operation * is linear in both of its operands. \
\
are the solutions for x, linear in t.\
x(t) as a solution to these two equations. x(ct) = cx(t)\
x(ct) implies that dF/du(ct) * x = 0 -> c*dF/du(t) * x = 0 -> dF/du(t) * cx = 0\
\
A(t) a linear function of t.\
are solutions for x to the equation A(t) * x = v linear in t ?\
in order for this to be so we need to prove two things:\
\
if b is a solution to A(t)*x = v then cb is a solution for A(ct)*x = v\
if b1, b2 are solutions to A(t1)*x = v and A(t2)*x = v respectively then b1+b2 is a solution to A(b1t+b2t)*x = v\
\
so A(t)*b = v -> A(ct)*cb = v that's not right at all!\
\
\
the only problem now is that x(t) is not well defined, representing only a line in space.\
There needs to be one more equation, something like  [linear function of t]*x = [other linear function of t]\
\
that leaves a lot of options.  The solution exists and is non-zero, so if A*x = b with A, b functions of t .  A and b must be non zero for all important t, and for all t the matrix formed by dF/du, dF/dv and A must be non-degenerate.\
\
the vector formed by interpolating the normals at either end of a curve might have this property, particularly since the partial derivatives are not allowed to be parallel, so no linear combination of them can be 0.  The other requirement is that it always be linearly independent of the two partial derivatives.  That is going to be harder to prove.  The definition of linear independence would require that X*dF/du(t) + Y*dF/dv(t) = z -> X = Y = 0\
z = [dF/du(0) x dF/dv(0)] (1-t) + [dF/du(1) x dF/dv(1)] t \
\
the vector will be formed by interpolating normals is N(0,0) + ( N(1,0) - N(0,0) )*u + ( N(0, 1) - N(0,0) )* v\
the scalar will be 1.\
\
\
( [?]*u + [?]*v + [?]  )\
\
is inversion a linear function? it would seem not :\
(X*c)^-1 = 1/c * (X)^-1 right?  So how can I claim that the solutions to the equation A(t) * x = [0, 0, 1]  is a linear function of t.\
\
We would want them to take the form  A'(t)   so   A(t) ( A'(t) ) = [0,0,1] for all t.  \
So lets compute one row and see how that comes out\
     [A00(t) A01(t)  A02(t)]         [  A'00 A'01 A'02 ]        [u]           [0]\
     [A10(t) A11(t)  A(12(t)]  *  ( [ A'10  A'11 A'12 ]   *   [v]  )   =  [0]\
     [A20(t) A21(t)  A(22(t)]        [ A'20  A'21 A'22 ]        [1]          [1]\
     \
->\
\
                     [ A'00 u + A'01 v + A'02 ]        [0]\
A(u,v,1) * (  [ A'10 u + A'11 v + A'12 ]  ) = [0]\
                     [ A'20 u + A'21 v + A'22 ]         [1]\
->\
\
A00(t) * ( A'00 u + A'01 v + A'02 ) + A01(t) * (A'10 u + A'11 v + A'12 )  + A02(t) * (A'20 u + A'21 v + A'22) = 0  \
for all t.  In particular, consider the basis (0 0 1) (1 0 1) (0 1 1).  Each of these will yield a distinct equation, 3 for the first row of the matrix.\
\
a plane can be represented as an operator that takes a 2-vector and returns a 3-vector.\
a line takes a 1-vector and returns a 3-vector.\
\
How to compute the intersection of two planes, given operators for both.\
A, B operators 2->3\
the condition is Ax = Bx for some x.\
\
the answer is to take an arbitrary vector from one and project it onto the other, by adding projections of two basis vectors onto each other.\
\
the plane perpendicular to the vector x is defined by x*y = 0.  The solutions to this equation form a vector space, so we really only need two linearly independent solutions to the equation x1*y1 + x2*y2 + x3*y3 = 0.  preferably as linear functions of the parameters u,v since the xs are all linear functions of u,v.\
\
x1 = x1u*u + x1v*v ...\
\
consider the vector (-x2, x1, 0) and (-x3, 0, x1)  these work, unless 2 out of the components are 0.  We know they are not all 0,\
so perhaps something like\
        (  -x2-x3, x1+x3, x2-x1 ) and ( x2-x3, -x1-x3, x1+x2 )\
but that doesn't help us much either, since the very assumption that x*y = 0 means that the operator x is degenerate, or the space is null !\
\
consider the operator.  It is degenerate, as the rows are not linearly indelendent.  row 1 + row 2 = -row 3 what does the space \
[  0	-1	-1 ]										      \
[  1	 0	 1 ] * x = 0 look like ?  The rank of the operator is 2, so this will be a 1 dimensional space.  \
[ -1	 1	 0 ]\
\
what we need is\
[  0	0	 0 ]	[ 1 ]		[ x ]\
[  0	0	 0 ] *	[ 1 ]   =  	[ y ]\
[  0	0	 0 ]	[ 1 ]	        +	[ z ]\
				          -------\
no way!				  0\
\
lets call one derivative	[  F/du	F/du	F/du ]				[  F/dv	F/dv	F/dv ]\
				[  F/du F/du    F/du ] , and the other	[  F/dv	F/dv	F/dv ]\
                			[  F/du F/du    F/du ]				[  F/dv	F/dv	F/dv ]\
\
now the desire is to have a linear function of u, v which satisfy the equations\
				[  X	X	X ]   	[ u ]		[ F/du	F/du	F/du ]       [u]\
			(	[  X	X	X ] *	[ v ]    ) * (        [ F/du F/du     F/du ]   *  [v] )   = 0 (and same for F/dv )\
				[  X	X	X ]	[ H ]		[ F/du F/du	F/du ]	     [H]\
\
so the form of that equation would be\
				[ Xu + Xv + X]	[uF/du + vF/du+ F/du]\
				[ Xu + Xv + X]  *     [uF/du + vF/du +  F/du]  = 0 ->\
				[ Xu + Xv + X]	[uF/du + vF/du + F/du]\
\
				for all u,v\
				(Xu + Xv + XH) * [ uF/du + vF/du + HF/du ] +\
				... = 0\
\
we can easily generate 9 equations, but we must hope that the resulting matrix is degenerate, because otherwise the proposition is ridiculous \
what would our equations look like ?\
\
(X*dF/du + X*dF/du + X* dF/du )   u * u\
		"			u * v\
					u * H\
					v  * v\
					v * H\
					H * H\
\
\
The second term looks like :\
[ dF/dv ]	[ u]		[dF/du]    [ u ]	\
	 	[ v]     *  		    [ v  ]	\
		[ 1]		                [ 1 ]		     \
\
=  (dF/dv*u + dF/dv*v + dF/dv)*(dF/du*u + dF/du*v + dF/du )\
\
\
what if I adopt a change of variables. \
\
Can I factor F into ( Ax + By + C) * ( Dx + Ey + F ) ?\
I am allowed to choose C and F to be anything, since that would just be a translation.\
In fact, I am allowed to modify the whole thing by a linear transformation aren't I?\
\
lets look at function as not being square, and remove the homogeneous coordinate.\
\
that would leave 3 equations for each derivative and the matrix would have 6 unknowns.  Which is spot on! bully!\
\
the only problem will be the origin which is forced to be 0, but I can just move it so that it is not on my triangle.\
\
[ a1	a2 ] [u] 	[ 2c0.0 c2.0 c3.0] [u]\
[ a3	a4 ] [v] * 	[ 2c0.1 c2.1 c3.1] [v] \
[ a5	a6 ]		[ 2c0.2 c2.2 c3.2] [1]\
\
[ a1*u + a2*v ]    [ 2c0.0*u +  c2.0*v +  c3.0 ]\
[ a3*u + a4*v ] * [ 2c0.1*u + c2.1*v + c3.1 ]\
[ a5*u + a6*v ]   [ 2c0.2*u + c2.2*v + c3.2 ]\
\
\
[ a1	a2 ] [u]	[ c2.0	2c1.0	c4.0] [u]\
[ a3	a4 ] [v] * 	[ c2.1	2c1.1	c4.1] [v]\
[ a5	a6 ] 		[ c2.2	2c1.2	c4.2] [1]\
\
[ a1*u + a2*v ]    [ c2.0*u + 2c1.0*v + c4.0 ]\
[ a3*u + a4*v ] * [ c2.1*u + 2c1.1*v + c4.1 ]\
[ a5*u + a6*v ]    [ c2.2*u + 2c1.2*v + c4.2 ]\
\
I need to quash the homogeneous coordinate on the derivatives, I will do that by changing variables so that the two lines where the derivatives are 0 intersect.  Solve the equation :\
\
2u*c0 + v*c2 + c3 = 0  this equation can only be solved if c0 c2 and c3 are linearly dependent. \
u*c2 + 2v*c1 + c4 = 0 so it is likely that the derivative is nowhere 0.\
\
What if we assume that the homogeneous coordinate is a linear function of the other two, rather than just constant.\
Might it not be possible to modify the derivative vector, since all we need is the direction.  \
\
the derivative vector is a linear function.  u*C0 + v*C1 + C2  ;  if we scale this vector by a linear function of the two parameters (a*u + b*v ) we get\
a*C0*u*u + a*C1*v*u + a*C2*u + b*C0*v*u + b*C1*v*v + b*C2*v.  That is not very useful.\
\
suppose instead we wish to represent the derivative using a homogeneous coordinate other than constant.  In this case though, it would be 0 at the origin, even though the derivative usually isn't. \
\
In two dimensions, the function which finds a perpendicular to a given vector is  	[ 0 -1]  and this is very nice because we only need one vector.												[ 1 0 ]\
\
in three dimensions we need a vector perpendicular to a pair of vectors so the matrix will be 3 x 6, and have 18 unknowns.  The equation takes the form:\
 [ a1	a2	a3	a4	a5	a6	]    [ x1 ]     [x1]\
([ a7	a8	a9	a10	a11	a12 	] * [ y1 ] ) *[y1]     =   0  for all x1 y1 z1 x2 y2 z2.  36 equations for 18 unknowns\
 [ a13	a14	a15	a16	a17	a18	]    [ z1 ]     [z1]\
						     [ x2 ]\
						     [ y2 ]\
						     [ z2 ]	\
\
lets consider a similar problem in 2 dimensions, where we know the solution\
( [ a1	a2 ] * [ x ] ) * [ x ]   =  0    \
  [ a3	a4 ]    [ y ]      [ y ]\
\
[ a1*x + a2*y ] * [ x ]  =  0  for all x, y\
[ a3*x + a4*y ]    [ y ]\
\
a1*x^2 + a2*x*y + a3*x*y + a4*y^2 = 0 for all x,y\
\
this amounts to the assertion that :\
a1*x^2 = 0\
( a2 + a3 ) * x*y = 0\
a4*y^2 = 0\
\
so only 3 equations, one for each combination x, y.\
\
In the three dimensional case there are 	x1x1 	x1y1 	x1z1 	x1x2 	x1y2 	x1z2	6\
							y1y1	y1z1	y1x2	y1y2	y1z2	5\
								z1z1	z1x2	z1y2	z1z2	4\
												15 equations for each matrix. \
												most of these equations have only one variable in them, \
												so there is still hope.\
  \
[ x1a1+y1a2+z1a3+x2a4+y2a5+z2a6]			[x1]\
[ x1a7+y1a8+z1a9+x2a10+y2a11+z2a12]	*	[y1]	= 0\
[ x1a13+y1a14+z1a15+x2a16+y2a17+z2a18]             [z1]                                                           \
\
[ x1a1+y1a2+z1a3+x2a4+y2a5+z2a6]			[x2]\
[ x1a7+y1a8+z1a9+x2a10+y2a11+z2a12]	*	[y2]	= 0\
[ x1a13+y1a14+z1a15+x2a16+y2a17+z2a18]             [z2]                                                          \
\
for all x1 y1 z1 x2 y2 z2\
\
the only matrix which satisfies these equations is all 0s, but that is because we haven't tied them to the parameters u and v.\
\
what about solving for a basis to the plane that is perpendicular to the derivative.  A pair of 3x3 linear operators.  Again there are 18 variables.\
\
[ a1	a2	a3 	]\
[ a4	a5	a6 	]\
[ a7	a8	a9 	]	[ x ]\
[ a10	a11	a12 	]     *	[ y ]  \
[ a13	a14	a15	]	[ z ]\
[ a16	a17	a18	]\
\
[ a1x + a2y + a3z ]	[ x ]\
[ a4x + a5y + a6z ] * [ y ] = 0      equations : xx xy xz yy yz zz\
[ a7x + a8y + a9z ]	[ z ]\
\
[ a10*x + a11*y + a12*z ]	 [ x ]\
[ a13*x + a14*y + a15*z ]  * [ y ] = 0\
[ a16*x + a17*y + a18*z ]	 [ z ]\
\
[ a1*x + a2*y + a3*z ]    [ a10*x + a11*y + a12*z ] \
[ a4*x + a5*y + a6*z ] * [ a13*x + a14*y + a15*z ] = 0\
[ a7*x + a8*y + a9*z ]    [ a16*x + a17*y + a18*z ]\
\
for all x, y, z\
\
18 equations, 18 unknowns.\
\
a1xx + a2xy + a3xz +			a1 = a5 = a9 = 0\
a4xy + a5yy + a6yz +			a2+a4 = 0	a6+a8 = 0\
a7xz + a8yz + a9zz = 0			a3+a7 = 0\
\
a10xx + a11xy + a12xz +			a10 = a14 = a18 = 0\
a13xy + a14yy + a15yz +			a11+a13 = 0		a15+a17=0\
a16xz + a17yz + a18zz = 0		a12+a16 = 0\
\
a1*a10*xx + a2*a10*xy + a3*a10*xz +\
a1*a11*xy + a2*a11*yy + a3*a11*yz +\
a1*a12*xz + a2*a12*yz + a3*a12*zz +\
\
a4*a13*xx + a4*a14*xy + a4*a15*xz +\
a5*a13*xy + a5*a14*yy + a5*a15*yz +\
a6*a13*xz + a6*a14*yz + a6*a15*zz +\
\
a7*a16*xx + a7*a17*xy + a7*a18*xz +\
a8*a16*xy + a8*a17*yy + a8*a18*yz +\
a9*a16*xz + a9*a17*yz + a9*a18*zz  = 0\
\
(substitute the known 0s )\
\
0		+   0    +  0  +\
0		+ a2*a11*yy + a3*a11*yz +\
0		+ a2*a12*yz + a3*a12*zz +\
\
a4*a13*xx +  0		+ a4*a15*xz +\
0 +\
a6*a13*xz + 0		 + a6*a15*zz +\
\
a7*a16*xx + a7*a17*xy + 0 +\
a8*a16*xy + a8*a17*yy + 0 +\
0	 = 0\
\
so ...\
\
xx	a4*a13 + a7*a16 = 0\
xy	a7*a17 + a8*a16 = 0\
xz	a4*a15 + a6*a13 = 0\
yy	a2*a11 + a8*a17 = 0\
yz	a3*a11 + a2*a12 = 0\
zz	a6*a15 + a3*a12 = 0\
\
[ a1	a2	a3 	]\
[ a4	a5	a6 	]\
[ a7	a8	a9 	]	[ x ]\
[ a10	a11	a12 	]     *	[ y ]  \
[ a13	a14	a15	]	[ z ]\
[ a16	a17	a18	]\
\
[ 0	1	1 ]\
[-1	0	1 ]\
[-1	-1	0 ]\
\
[ 0	a	b ]\
[-a	0	c ]\
[-b	-c	0 ]\
\
can any choice make this matrix non-degenerate ?\
\
[ 0	a	b ]\
[ -a	0	c ]  \
[ -b	-c	0 ]\
\
[ -b	-c	0 ]\
[  0	a	b ]\
[ -a	0	c ]\
\
[ -b			-c		0 	]\
[ 0			a		b	]\
[ -a  + b * a / b  	c*a/b		c	]	+ row1 * -a/b\
\
[ -b			-c		0	]\
[ 0			a		b	]\
[ 0			c*a/b		c	]	+ row2 * -c/b\
\
okay, that was silly, because I had already come to the same conclusion by another route.\
hypothesis : the results of these matrixes are always coplanar, and the degeneracy is the line perpendicular to that plane.  This is of course true, because it is true of every degenerate matrix.  We are always projecting onto a particular plane.  Can we find this matrix from the vector to which the plane is perpendicular?\
\
the conditions are ay + bz = 0\
		         -ax + cz = 0\
		         -bx -  cy = 0\
			\
		 	a = cz/x\
			b = -cy/x\
			cyz/x -cyz/x = 0  <- so we can pick c\
			if we select c to b x then the matrix becomes\
\
[ 0	z	-y ]\
[-z	0	 x ]\
[ y	-x	 0 ]\
			\
this is the cross product with [ x y z ]\
\
suppose I select two vectors perpendicular to each derivative by crossing them with arbitrary reference vectors A, B.  We now have a basis for two planes, four vectors.  \
\
Is it possible to find a linear operator which transforms these 12 coordinates into 3, lying in the intersecting space.  That would be 36 variables!?!\
\
[ a1	a2	a3	a4	a5	a6	a7	a8	a9	a10	a11	a12 ]   [ x1 ]\
[ a13	a14	a15	a16	a17	a18	a19	a20	a21	a22	a23	a24 ] *[ y1 ]\
[ a25	a26	a27	a28	a29	a30	a31	a32	a33	a34	a35	a36 ]   [ z1 ]\
												[ x2 ]\
												[ y2 ]\
												[ z2 ]\
												[ x3 ]\
												[ y3 ]\
												[ z3 ]\
												[ x4 ]\
												[ y4 ]\
												[ z4 ]\
\
If we take an arbitrary vector, project it onto one plane and then project it onto the other, don't we get a vector at the intersection of the two planes.  It would seem not, how strange.\
\
given 4 vectors v1 v2 v3 v4  we want to find 4 constants A B C D such that\
Av1 + Bv2 = Cv3 + Dv4 or\
Av1 + Bv2 - Cv3 - Dv4 = 0\
or [ v1 v2 -v3 -v4 ] 	[ A ]    =   0 , 3 equations and 4 unknowns.\
			[ B ]\
			[ C ]\
			[ D ]\
\
two arbitrary projection matrices are selected\
[ 0	p1 	p2 ]		[ 0	p4	p5 ]\
[-p1 	0	p3 ]  and 	[-p4	0	p6 ]\
[ -p2	-p3	0   ]		[-p5	-p6	0   ]\
\
the vectors are partial derivatives of c0uu + c1vv + c2uv + c3u + c4v + c5\
applying each to \
\
	[ a1	a2	a3 ]	[ u ]		[ 2c0.0	c2.0	c3.0 ]	[ u ]\
       (	[ a4	a5	a6 ]  *	[ v ]   ) * (	[ 2c0.1	c2.1	c3.1 ]*	[ v ]	) = 0\
	[ a7	a8	a9 ]	[ H ]		[ 2c0.2	c2.2	c3.2 ]	[ H]\
\
	[ a1	a2	a3 ]	[ u ]		[ c2.0	2c1.0	c4.0 ]	[ u ]\
       (	[ a4	a5	a6 ]  *	[ v ]   ) * (	[ c2.1	2c1.1	c4.1 ]*	[ v ]	) = 0\
	[ a7	a8	a9 ]	[ H ]		[ c2.2	2c1.2	c4.2 ]	[ H]\
\
[ a1u + a2v + a3H ]		[ 2c0.0u + c2.0v + c3.0H ]\
[ a4u + a5v + a6H ]      * 	[ 2c0.1u + c2.1v + c3.1H ]	= 0		\
[ a7u + a8v + a9H ]		[ 2c0.2u + c2.2v + c3.2H ]\
\
[ a1u + a2v + a3H ]		[ c2.0u + 2c1.0v + c4.0H ]\
[ a4u + a5v + a6H ]      * 	[ c2.1u + 2c1.1v + c4.1H ]	= 0\
[ a7u + a8v + a9H ]		[ c2.2u + 2c1.2v + c4.2H ]\
\
uu	1:	a1*2c0.0 + a4*2c0.1 + a7*2c0.2 	= 0\
	2	a1*c2.0 + a4*c2.1 + a7*c2.2	= 0\
vv	3:	a2*c2.0 + a5*c2.1 + a8*c2.2	= 0\
	4	a2*2c1.0 + a5*c1.1 + a8*c1.2	= 0\
uv	5:	a1*c2.0 + a4*c2.1 + a7*c2.2 +	<- these terms are the same as equation 2\
		a2*2c0.0 + a5*2c0.1 + a8*2c0.2	= 0\
	6	a1*2c1.0 + a4*2c1.1 + a7*2c1.2 +\
		a2*c2.0 + a5*c2.1 + a8*c2.2	= 0   <-- these are the same as equation 3\
\
if we consider the matrix as a series of vectors :\
[ A1	A2	A3 ] x	* [ c0		c2	c3 ]x = 0\
[ A1	A2	A3 ] x	* [ c2		c1	c4 ]x = 0  for all x\
\
uu	1	A1*c0 = 0\
	2	A1*c2 = 0\
vv	3	A2*c2 = 0\
	4	A2*c1 = 0\
uv	5	A1*c2 + A2*c0 = 0   ->   A2*c0 = 0\
	6	A1*c1 + A2*c2 = 0	->   A1*c1 = 0\
uH	7	A1*c3 + A3*c0 = 0\
	8	A1*c4 + A3*c2 = 0\
vH	9	A2*c3 + A3*c2 = 0\
	10	A2*c4 + A3*c1 = 0\
HH	11	A3*c3 = 0\
	12	A3*c4 = 0\
\
by 1, 2	A1 = k1( c0 x c2 )\
by 3, 4	A2 = k2( c1 x c2 )\
by 11, 12	A3 = k3( c3 x c4 )\
by 5 A2 must be perpendicular to c0, which is perpendicular to both c1 and c2, so c1, c2 and c0 must be linearly dependent, ie lying in the same plane.  This is a substantial difficulty, since in general these \
}